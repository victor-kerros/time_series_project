# Projet de Séries temporelles linéaires - 2A ENSAE

## Partie I : Données et stationnarisation

### Q1 - Présentation de la série

``` {r import packages}

# on utilise "zoo" pour formaliser les séries temporelles
#install.packages("zoo")
require(zoo)

# on utilise "tseries" pour diverses fonctions
#install.packages("tseries")
require(tseries)

# on utilise "fUnitRoots" pour les tests de racine unitaire
#install.packages("fUnitRoots")
library(fUnitRoots)

# on utilise "forecast" pour les prédictions
#install.packages("forecast")
require("forecast")
```

Link : https://www.insee.fr/fr/statistiques/serie/010537313

``` {r import dataset}
datafile <- "engrais_valeurs_mensuelles.csv"
data <- read.csv(datafile, sep=";")
```

``` {r create dates}
string_dates <- as.character(data$dates)
# premiere et la derniere date pour définir les indices des dates
string_dates[1]; string_dates[length(string_dates)]
# on définit les dates de la série
dates <- as.yearmon(seq(from=1990+1/12, to=2022+3/12, by=1/12))
```

``` {r create ts fertilizer and dfertilizer with zoo}
# on permute la série sinon elle n'est pas dans le bon sens
fertilizer.source <- zoo(rev(data$indices), order.by=dates)
# on enlève les 6 dernières dates à la série en niveau serie
# cela permettra de tester avec le RMSE les différents modèles concurrents
fertilizer <- fertilizer.source[1:(length(fertilizer.source)-6)]
# delec est la série en différence première
dfertilizer.source <- diff(fertilizer.source, 1)
dfertilizer <- diff(fertilizer, 1)
```

```{r plot and decompose fertilizer.source}
plot(fertilizer.source, type="l", lwd=1, col="blue", main="fertilizer.source", xlab="dates", ylab="values" )
decompose_fertilizer.source <- decompose(fertilizer.source)
plot(decompose_fertilizer.source)
```

### Q2 - Stationnarisation de la série

On constate plusieurs choses : une tendance et de l'hétéroscédasticité.

Pour rendre la série stationnaire, on lisse d'abord l'hétéroscédasticité en passant au log.

``` {r log}
# on passe au log pour lisser l'hétéroscédasticité
logfertilizer.source <- log(fertilizer.source)
decompose_logfertilizer.source <- decompose(logfertilizer.source)
plot(decompose_logfertilizer.source)
plot(logfertilizer.source, type="l", lwd=1, col="blue", main="logfertilizer.source", xlab="dates", ylab="values")
```

### Q2 - Stationnarisation de la série

On constate sur le graphique ci-dessus une tendance linéaire positive pour la série en niveau.

On vérifie d'abord la tendance linéaire en effectuant une régression linéaire des valeurs de la série tronquée sur les dates.

```{r lm}
truncated_dates <- dates[1:(length(logfertilizer.source)-6)]
logfertilizer <- logfertilizer.source[1:(length(logfertilizer.source)-6)]
summary(lm(logfertilizer ~ truncated_dates))
```

Cette régression linéaire confirme la présence d'une tendance linéaire positive dans nos données et d'une constante : p_values < 1 %.

On cherche à déterminer si la série est stationnaire ou non. Pour cela, on souhaite effectuer un test de racine unitaire ADF dont l'hypothèse H1 est la stationnarité de la série. On effectue bien le test ADF dans le cas avec constante et tendance.

```{r logfertilizer adf test}
# test ADF dans le cas avec constante et tendance
adf_logfertilizer <- adfTest(logfertilizer, lag=0, type="ct")
adf_logfertilizer
```

D'après le test ADF, la série en niveau est stationnaire. Seulement, pour que le test soit valide, il faut que les résidus de la régression ne soient pas autocorrélés.
On teste donc l’autocorrélation des résidus dans la régression sur une période de deux ans.

``` {r test residuals autocorrelation : cf. TD4}

Qtests <- function(series, k, fitdf=0){
  pvals <- apply(matrix(1:k), 1, FUN=function(l){
  pval <- if (l<=fitdf) NA else Box.test(series, lag=l, type="Ljung-Box", fitdf=fitdf)$p.value
  return(c("lag"=l,"pval"=pval))
  })
  return(t(pvals))
}

Qtests(adf_logfertilizer@test$lm$residuals, 24, length(adf_logfertilizer@test$lm$coefficients))
```

L’absence d’autocorrélation des résidus est rejetée, le test ADF avec aucun
retard n’est donc pas valide. On ajoute alors des retards jusqu’à ce que les résidus ne soient plus autocorrélés à l'aide de la fonction ci-dessous.

```{r logfertilizer adf test : cf. TD5}
adfTest_valid <- function(series,kmax,type){ 
# tests ADF jusqu’à des résidus non autocorrélés
k <- 0
noautocorr <- 0
while (noautocorr==0){
cat(paste0("ADF with ",k, " lags: residuals OK? "))
adf <- adfTest(series,lags=k,type=type)
pvals <- Qtests(adf@test$lm$residuals,24,fitdf=length(adf@test$lm$coefficients))[,2]
if (sum(pvals<0.05,na.rm=T) == 0) {
noautocorr <- 1; cat("OK \n")}
else cat("nope \n")
k <- k + 1
}
return(adf)
}

adf <- adfTest_valid(logfertilizer, 24, "ct")
adf
```

On constate alors que la série en niveau n'est pas stationnaire : le test ADF ne rejette pas l'hypothèse racine unitaire (p-value > 0.05) i.e. de non stationnarité de la série. 
On passe à la série en différence première ou intégrée d'ordre 1.

```{r dlogfertilizer adf test}
dlogfertilizer.source <- diff(logfertilizer.source, 1)
dlogfertilizer <- diff(logfertilizer, 1)
plot(dlogfertilizer.source, type="l", lwd=1, col="blue", main="dlogfertilizer.source", xlab="dates", ylab="values")
summary(lm(dlogfertilizer ~ dates[2:(length(logfertilizer.source)-6)]))
# il n'y a pas de tendance ni de constante
adf_dlogfertilizer <- adfTest(dlogfertilizer, lag=0, type="nc")
Qtests(adf_dlogfertilizer@test$lm$residuals, 24, length(adf_dlogfertilizer@test$lm$coefficients))
```

Non seulement, la série intégrée d'ordre 1 ne présente pas de tendance et de constante significative mais en plus elle vérifie l’absence autocorrélation des résidus avec seize retards ce qui permet d'effectuer le test de Dickey Fuller.

```{r dlogfertilizer adf test}
adf <- adfTest_valid(dlogfertilizer, 24, "nc")
adf
```

Le test ADF n'est pas rejeté avec cinq retards pour la série différenciée, on retient son hypothèse alternative : la série est stationnaire. 
On détermine désormais pmax et qmax pour évaluer le modèle ARIMA(p,1,q) adéquat.

### Q3 - Représentation de la série avant et après transformation

``` {r plot fertilizer and dlogfertilizer}
plot(cbind(fertilizer.source,dlogfertilizer.source))
```

## Partie 2 : Modèles ARMA

### Q4 - Choix du modèle ARMA

On détermine d'abord pmax et qmax.

``` {r acf and pacf dlogfertilizer}
acf(dlogfertilizer); pacf(dlogfertilizer)
```

On ignore les autocorrélations (partielles) pour les retards supérieurs strictement à 15 dans l'objectif d'obtenir un modèle raisonnablement simplifié.
Dans notre cas, il n'y en a pas quand bien même on note la présence d'autocorrélations persistantes mais non significatives tous les six mois.

``` {r pmax and qmax}
pmax=15; qmax=1
```



``` {r find valid models}

# fonction de test des significativités individuelles des coefficients
signif <- function(estim){
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1-pnorm(abs(t)))*2
  return(rbind(coef,se,pval))
}

# fonction pour estimer un arima et en vérifier l’ajustement et la validité
modelchoice <- function(p, q, data=dlogfertilizer, k=24){
  estim <- try(arima(data, c(p,0,q),optim.control=list(maxit=20000)))
  if (class(estim)=="try-error") return(c("p"=p,"q"=q,"arsignif"=NA,"masignif"=NA,"resnocorr"=NA, "ok"=NA))
  arsignif <- if (p==0) NA else signif(estim)[3,p]<=0.05
  masignif <- if (q==0) NA else signif(estim)[3,p+q]<=0.05
  resnocorr <- sum(Qtests(estim$residuals,24,length(estim$coef)-1)[,2]<=0.05,na.rm=T)==0
  checks <- c(arsignif,masignif,resnocorr)
  ok <- as.numeric(sum(checks,na.rm=T)==(3-sum(is.na(checks))))
  return(c("p"=p,"q"=q,"arsignif"=arsignif,"masignif"=masignif,"resnocorr"=resnocorr,"ok"=ok))
}


# fonction pour estimer et vérifier tous les arma(p,q) (p<=pmax ; q<=qmax)
armamodelchoice <- function(pmax,qmax){
  pqs <- expand.grid(0:pmax,0:qmax) ; t(apply(matrix(1:dim(pqs)[1]),1,function(row) {
    p <- pqs[row,1]; q <- pqs[row,2]
    cat(paste0("Computing ARMA(",p,",",q,") \n"))
    modelchoice(p,q)
  }))}

armamodels <- armamodelchoice(pmax,qmax)

# on ne retient que les modèles ajustés et valides
selec <- armamodels[armamodels[, "ok"]==1&!is.na(armamodels[, "ok"]),]
selec
```

On obtient 5 modèles valides et justifiés. On évalue la qualité de ces modèles avec les critères AIC et BIC.

``` {r AIC and BIC}

# on crée la liste des arma(p,q) possibles
pqs <- apply(selec,1,function(row)
list("p"=as.numeric(row[1]), "q"=as.numeric(row[2])))
names(pqs) <- paste0("arma(", selec[,1], ",", selec[,2],")")

# on crée la liste des modèles arma(p,q)
models <- lapply(pqs, function(pq) arima(dlogfertilizer, c(pq[["p"]], 0, pq[["q"]])))

# on calcule les AIC et BIC des modèles possibles
vapply(models, FUN.VALUE=numeric(2), function(m) c("AIC"=AIC(m),"BIC"=BIC(m)))
```

On compare les prédictions des modèles arma(2,1) (minimise l'AIC) et arma(1,1) (minimise le BIC) avec le RMSE.

``` {r select model with best rmse}

# définition des deux modèles concurrents
ar1ma2 <- arima(fertilizer, c(1,1,2), include.mean=F)
ar1ma1 <- arima(fertilizer, c(1,1,1), include.mean=F)

# on crée des séries pour les prédictions des deux modèles à tester
models <- c("ar1ma2","ar1ma1")
preds <- zoo(matrix(NA, ncol=2, nrow=6), order.by=tail(index(fertilizer.source), 6))
colnames(preds) <- models
logfertilizerp <- preds

# on remplit la série avec les prédictions des modèles
for (model in models){
pred <- zoo(predict(get(model), 6)$pred, order.by=tail(index(fertilizer.source), 6))
logfertilizerp[, model] <- pred
}

# on affiche les valeurs observées et les prédictions
obs <- tail(fertilizer.source, 6)
cbind(obs, logfertilizerp)

# on calcule les rmse
apply(logfertilizerp, 2, function(x) sqrt(sum((x-obs)^2)/6)/sd(fertilizer.source))
```

### Q5 - Expression du modèle ARIMA retenu.

``` {r selected model}
selected_model <- ar1ma1
selected_model
```

On vérifie la significativité du coefficient.

``` {r check selected model}

# test de significativité des coefficients
tar1ma1 <- ar1ma1$coef / sqrt(diag(ar1ma1$var.coef))
pvar1ma1 <- (1-pnorm(abs(tar1ma1)))*2
pvar1ma1["ma1"]
pvar1ma1["ar1"]

# test de non auto-corrélation des résidus
Qtests(ar1ma1$residuals, 12, fitdf=1)

```

## Partie III : Prévisions

### Q6 - Région de confiance pour les valeurs futures t+1 et t+2

### Q7 - Hypothèses pour la région de confiance

### Q8 - Représentation graphique de la région de confiance

``` {r forecasts}
forecasts <- forecast(ar1ma1, h=6, level=0.95)
plot(forecasts, xlim=c(2018,2022), shadecols = "grey", fcol="red", main = "intervalle de confiance (alpha = 0.95) pour T+1 et T+2")
par(new=T)
plot(fertilizer.source, type="l", lwd=1, col="blue", xlim=c(2018,2022))
```

### Q9 - Question ouverte


