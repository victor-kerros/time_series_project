# Projet de Séries temporelles linéaires - 2A ENSAE

## Partie I : Données et stationnarisation

### Q1 - Présentation de la série

``` {r import packages}

# on utilise "zoo" pour formaliser les séries temporelles
#install.packages("zoo")
require(zoo)

# on utilise "tseries" pour diverses fonctions
#install.packages("tseries")
require(tseries)

# on utilise "fUnitRoots" pour les tests de racine unitaire
#install.packages("fUnitRoots")
library(fUnitRoots)

# on utilise "forecast" pour les prédictions
#install.packages("forecast")
require("forecast")
```

Link : https://www.insee.fr/fr/statistiques/serie/010537313

``` {r import dataset}
datafile <- "clothes_valeurs_mensuelles.csv"
data <- read.csv(datafile, sep=";")
```

``` {r create dates}
string_dates <- as.character(data$dates)
# premiere et la derniere date pour définir les indices des dates
string_dates[1]; string_dates[length(string_dates)]
# on définit les dates de la série
dates <- as.yearmon(seq(from=1990+1/12, to=2022+3/12, by=1/12))
```

``` {r create ts wine and dwine with zoo}
# on permute la série sinon elle n'est pas dans le bon sens
wine.source <- zoo(rev(data$indices), order.by=dates)
# on enlève les 6 dernières dates à la série en niveau serie
wine <- wine.source[1:(length(wine.source)-6)]
# delec est la série en différence première
dwine.source <- diff(wine.source, 1)
dwine <- diff(wine, 1)
```

```{r plot and decompose wine.source}
plot(wine.source, type="l", lwd=1, col="blue", main="wine.source", xlab="dates", ylab="values" )
decompose_wine.source <- decompose(wine.source)
plot(decompose_wine.source)
```

### Q2 - Stationnarisation de la série

On constate plusieurs choses : une tendance et de l'hétéroscédasticité.

Pour rendre la série stationnaire, on lisse d'abord l'hétéroscédasticité en passant au log.

``` {r log}
# on passe au log pour lisser l'hétéroscédasticité
logwine.source <- log(wine.source)
decompose_logwine.source <- decompose(logwine.source)
plot(decompose_logwine.source)
plot(logwine.source, type="l", lwd=1, col="blue", main="logwine.source", xlab="dates", ylab="values")
```

### Q2 - Stationnarisation de la série

On constate sur le graphique ci-dessus une tendance linéaire positive pour la série en niveau.

On vérifie d'abord la tendance linéaire en effectuant une régression linéaire des valeurs de la série tronquée sur les dates.

```{r lm}
truncated_dates <- dates[1:(length(logwine.source)-6)]
logwine <- logwine.source[1:(length(logwine.source)-6)]
summary(lm(logwine ~ truncated_dates))
```

Cette régression linéaire confirme la présence d'une tendance linéaire positive dans nos données et d'une constante : p_values < 1 %.

On cherche à déterminer si la série est stationnaire ou non. Pour cela, on souhaite effectuer un test de racine unitaire ADF dont l'hypothèse H1 est la stationnarité de la série. On effectue bien le test ADF dans le cas avec constante et tendance.

```{r logwine adf test}
# test ADF dans le cas avec constante et tendance
adf_logwine <- adfTest(logwine, lag=0, type="ct")
adf_logwine
#pp.test(wine)
```

D'après le test ADF, la série en niveau est stationnaire. Seulement, pour que le test soit valide, il faut que les résidus de la régression ne soient pas autocorrélés.
On teste donc l’autocorrélation des résidus dans la régression sur une période de deux ans.

``` {r test residuals autocorrelation : cf. TD4}

Qtests <- function(series, k, fitdf=0){
  pvals <- apply(matrix(1:k), 1, FUN=function(l){
  pval <- if (l<=fitdf) NA else Box.test(series, lag=l, type="Ljung-Box", fitdf=fitdf)$p.value
  return(c("lag"=l,"pval"=pval))
  })
  return(t(pvals))
}

Qtests(adf_logwine@test$lm$residuals, 24, length(adf_logwine@test$lm$coefficients))
```

L’absence d’autocorrélation des résidus est rejetée, le test ADF avec aucun
retard n’est donc pas valide. On ajoute alors des retards jusqu’à ce que les résidus ne soient plus autocorrélés à l'aide de la fonction ci-dessous.

```{r logwine adf test : cf. TD5}
adfTest_valid <- function(series,kmax,type){ 
# tests ADF jusqu’à des résidus non autocorrélés
k <- 0
noautocorr <- 0
while (noautocorr==0){
cat(paste0("ADF with ",k, " lags: residuals OK? "))
adf <- adfTest(series,lags=k,type=type)
pvals <- Qtests(adf@test$lm$residuals,24,fitdf=length(adf@test$lm$coefficients))[,2]
if (sum(pvals<0.05,na.rm=T) == 0) {
noautocorr <- 1; cat("OK \n")}
else cat("nope \n")
k <- k + 1
}
return(adf)
}

adf <- adfTest_valid(logwine, 24, "ct")
adf
```

wine = 14 lags
pharm = 11 lags
alim = 8 lags
elec = 21 lags
biere = 9 lags
petrole = 12 lags
gas = 4 lags
rocks = 12 lags
amy = 2 lags
water = 11 lags
clothes = 14 lags

On constate alors que la série en niveau n'est pas stationnaire : le test ADF ne rejette pas l'hypothèse racine unitaire (p-value > 0.05). 
On passe à la série en différence première ou intégrée d'ordre 1.

```{r dlogwine adf test}
dlogwine.source <- diff(logwine.source, 1)
dlogwine <- diff(logwine, 1)
plot(dlogwine.source, type="l", lwd=1, col="blue", main="dlogwine.source", xlab="dates", ylab="values")
summary(lm(dlogwine ~ dates[2:(length(logwine.source)-6)]))
# il n'y a pas de tendance ni de constante
adf_dlogwine <- adfTest(dlogwine, lag=0, type="nc")
Qtests(adf_dlogwine@test$lm$residuals, 24, length(adf_dlogwine@test$lm$coefficients))
adf_dlogwine
```

Non seulement, la série intégrée d'ordre 1 ne présente pas de tendance et de constante significative mais en plus elle vérifie l’absence autocorrélation des résidus à l'ordre 1.

```{r dlogwine adf test}
adf <- adfTest_valid(dlogwine, 24, "nc")
adf
```

rocks : 7 lags
pharm : 10 lags
alim : 6 lags
elec : 12 lags
wine = 15 lags
biere = 8 lags
petrole = 10 lags
spirit = 7 lags
gas = 3 lags
amy = 1 lag
water = 8 lags
clothes = 16 lags

Le test ADF n'est pas rejeté avec un retard pour la série différenciée, on retient son hypothèse alternative : la série est stationnaire. On détermine désormais pmax et qmax pour déterminer le modèle ARIMA.

### Q3 - Représentation de la série avant et après transformation

``` {r plot wine and dlogwine}
plot(cbind(wine.source,dlogwine.source))
```

## Partie 2 : Modèles ARMA

### Q4 - Choix du modèle ARMA

On détermine d'abord pmax et qmax.

``` {r acf and pacf dlogwine}
acf(dlogwine); pacf(dlogwine)
```

qamy = 3
pamy = 1

qmilk = 6 (15)
pmilk = 6 (14)

qjuices = 15
pjuices = 13

qwater = 8
pwater = 9

qrocks = 1
procks = 8

qgas = 4
pgas = 3

qpharm = 12
ppharm = 11

qspirit = 12
pspirit = 12

qpetrole = 13
ppetrole = 11

qwine = 14 (4)
pwine = 14 (3)

qelec = 24 (7)
pelec = 24 (7)

qalim = 24 (2 / 7)
palim = 7

qbiere = 2
pbiere = 8

qclothes = 7
pclothes = 9

qtextile = 2
ptextile = 4

On ignore les autocorrélations (partielles) pour les retards supérieurs à 20 dans l'objectif d'obtenir un modèle le plus simple possible.

``` {r pmax and qmax}
pmax=9; qmax=7
```

``` {r find valid models}

# fonction de test des significativités individuelles des coefficients
signif <- function(estim){
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1-pnorm(abs(t)))*2
  return(rbind(coef,se,pval))
}

# fonction pour estimer un arima et en vérifier l’ajustement et la validité
modelchoice <- function(p, q, data=dlogwine, k=24){
  estim <- try(arima(data, c(p,0,q),optim.control=list(maxit=20000)))
  if (class(estim)=="try-error") return(c("p"=p,"q"=q,"arsignif"=NA,"masignif"=NA,"resnocorr"=NA, "ok"=NA))
  arsignif <- if (p==0) NA else signif(estim)[3,p]<=0.05
  masignif <- if (q==0) NA else signif(estim)[3,p+q]<=0.05
  resnocorr <- sum(Qtests(estim$residuals,24,length(estim$coef)-1)[,2]<=0.05,na.rm=T)==0
  checks <- c(arsignif,masignif,resnocorr)
  ok <- as.numeric(sum(checks,na.rm=T)==(3-sum(is.na(checks))))
  return(c("p"=p,"q"=q,"arsignif"=arsignif,"masignif"=masignif,"resnocorr"=resnocorr,"ok"=ok))
}


# fonction pour estimer et vérifier tous les arma(p,q) (p<=pmax ; q<=qmax)
armamodelchoice <- function(pmax,qmax){
  pqs <- expand.grid(0:pmax,0:qmax) ; t(apply(matrix(1:dim(pqs)[1]),1,function(row) {
    p <- pqs[row,1]; q <- pqs[row,2]
    cat(paste0("Computing ARMA(",p,",",q,") \n"))
    modelchoice(p,q)
  }))}

armamodels <- armamodelchoice(pmax,qmax)

# on ne retient que les modèles ajustés et valides
selec <- armamodels[armamodels[, "ok"]==1&!is.na(armamodels[, "ok"]),]
selec
```

``` {r AIC and BIC}

# on crée la liste des arma(p,q) possibles
pqs <- apply(selec,1,function(row)
list("p"=as.numeric(row[1]), "q"=as.numeric(row[2])))
names(pqs) <- paste0("arma(", selec[,1], ",", selec[,2],")")

# on crée la liste des modèles arma(p,q)
models <- lapply(pqs, function(pq) arima(dlogwine, c(pq[["p"]], 0, pq[["q"]])))

# on calcule les AIC et BIC des modèles possibles
vapply(models, FUN.VALUE=numeric(2), function(m) c("AIC"=AIC(m),"BIC"=BIC(m)))
```

On compare les prédictions des modèles arma(1,1) (minimise l'AIC) et arma(0,1) (minimise le BIC) avec le RMSE.

``` {r select model with best rmse}

#ar1ma1 <- arima(logwine, c(9,1,7), optim.control=list(maxit=20000))
#ma1 <- arima(logwine, c(7,1,0), optim.control=list(maxit=20000))
ar1ma1 <- arima(logwine, c(0,1,7), optim.control=list(maxit=20000))
ma1 <- arima(logwine, c(5,1,0), optim.control=list(maxit=20000))

# on crée des séries pour les prédictions des deux modèles à tester
models <- c("ar1ma1","ma1")
preds <- zoo(matrix(NA, ncol=2, nrow=6), order.by=tail(index(logwine.source), 6))
colnames(preds) <- models
logwinep <- preds

# on remplit la série avec les prédictions des modèles
for (model in models){
pred <- zoo(predict(get(model), 6)$pred, order.by=tail(index(logwine.source), 6))
logwinep[, model] <- pred
}

# on affiche les valeurs observées et les prédictions
obs <- tail(logwine.source, 6)
cbind(obs, logwinep)

# on calcule les rmse
apply(logwinep, 2, function(x) sqrt(sum((x-obs)^2)/6)/sd(logwine.source))
```

### Q5 - Expression du modèle ARIMA retenu.

``` {r selected model}
selected_model <- ma1
selected_model
```

On vérifie la significativité du coefficient.

``` {r check selected model}
arima011 <- arima(logwine, c(9,1,7), include.mean=F)
tarima011 <- arima011$coef / sqrt(diag(arima011$var.coef))
pvarima011 <- (1-pnorm(abs(tarima011)))*2
pvarima011["ma7"]
pvarima011["ar9"]
Qtests(arima011$residuals, 24, fitdf=1)
```

## Partie III : Prévisions

### Q6 - Région de confiance pour les valeurs futures t+1 et t+2

### Q7 - Hypothèses pour la région de confiance

### Q8 - Représentation graphique de la région de confiance

``` {r forecasts}
forecasts <- forecast(ma1, h=2, level=c(95))
plot(forecasts, , xlim=c(2018,2022), shadecols = "grey", fcol="blue", main = "Intervalles de confiance à 95% pour T+1 et T+2")
```

``` {r forecasts}
prev <- forecast(arima011, h=6, level=0.95)
plot(prev, xlim=c(2017,2022))
lines(dlogwine.source[1:386], xlim=c(2017,2022))
```

### Q9 - Question ouverte


