# Projet de Séries temporelles linéaires - 2A ENSAE

## Partie I : Données et stationnarisation

### Q1 - Présentation de la série

``` {r import packages}

# on utilise "zoo" pour formaliser les séries temporelles
#install.packages("zoo")
require(zoo)

# on utilise "tseries" pour diverses fonctions
#install.packages("tseries")
require(tseries)

# on utilise "fUnitRoots" pour les tests de racine unitaire
#install.packages("fUnitRoots")
library(fUnitRoots)

# on utilise "forecast" pour les prédictions
#install.packages("forecast")
require("forecast")
```

Link : https://www.insee.fr/fr/statistiques/serie/010537313

``` {r import dataset}
datafile <- "water_valeurs_mensuelles.csv"
data <- read.csv(datafile, sep=";")
```

``` {r create dates}
string_dates <- as.character(data$dates)
# premiere et la derniere date pour définir les indices des dates
string_dates[1]; string_dates[length(string_dates)]
# on définit les dates de la série
dates <- as.yearmon(seq(from=1990+1/12, to=2022+3/12, by=1/12))
```

``` {r create ts water and dwater with zoo}
# on permute la série sinon elle n'est pas dans le bon sens
water.source <- zoo(rev(data$indices), order.by=dates)
# on enlève les 6 dernières dates à la série en niveau serie
# cela permettra de tester avec le RMSE les différents modèles concurrents
water <- water.source[1:(length(water.source)-6)]
# delec est la série en différence première
dwater.source <- diff(water.source, 1)
dwater <- diff(water, 1)
```

```{r plot and decompose water.source}
plot(water.source, type="l", lwd=1, col="blue", main="water.source", xlab="dates", ylab="values" )
decompose_water.source <- decompose(water.source)
plot(decompose_water.source)
```

### Q2 - Stationnarisation de la série

On constate plusieurs choses : une tendance et de l'hétéroscédasticité.

Pour rendre la série stationnaire, on lisse d'abord l'hétéroscédasticité en passant au log.

``` {r log}
# on passe au log pour lisser l'hétéroscédasticité
logwater.source <- log(water.source)
decompose_logwater.source <- decompose(logwater.source)
plot(decompose_logwater.source)
plot(logwater.source, type="l", lwd=1, col="blue", main="logwater.source", xlab="dates", ylab="values")
```

### Q2 - Stationnarisation de la série

On constate sur le graphique ci-dessus une tendance linéaire positive pour la série en niveau.

On vérifie d'abord la tendance linéaire en effectuant une régression linéaire des valeurs de la série tronquée sur les dates.

```{r lm}
truncated_dates <- dates[1:(length(logwater.source)-6)]
logwater <- logwater.source[1:(length(logwater.source)-6)]
summary(lm(logwater ~ truncated_dates))
```

Cette régression linéaire confirme la présence d'une tendance linéaire positive dans nos données et d'une constante : p_values < 1 %.

On cherche à déterminer si la série est stationnaire ou non. Pour cela, on souhaite effectuer un test de racine unitaire ADF dont l'hypothèse H1 est la stationnarité de la série. On effectue bien le test ADF dans le cas avec constante et tendance.

```{r logwater adf test}
# test ADF dans le cas avec constante et tendance
adf_logwater <- adfTest(logwater, lag=0, type="ct")
adf_logwater
#pp.test(water)
```

D'après le test ADF, la série en niveau est stationnaire. Seulement, pour que le test soit valide, il faut que les résidus de la régression ne soient pas autocorrélés.
On teste donc l’autocorrélation des résidus dans la régression sur une période de deux ans.

``` {r test residuals autocorrelation : cf. TD4}

Qtests <- function(series, k, fitdf=0){
  pvals <- apply(matrix(1:k), 1, FUN=function(l){
  pval <- if (l<=fitdf) NA else Box.test(series, lag=l, type="Ljung-Box", fitdf=fitdf)$p.value
  return(c("lag"=l,"pval"=pval))
  })
  return(t(pvals))
}

Qtests(adf_logwater@test$lm$residuals, 24, length(adf_logwater@test$lm$coefficients))
```

L’absence d’autocorrélation des résidus est rejetée, le test ADF avec aucun
retard n’est donc pas valide. On ajoute alors des retards jusqu’à ce que les résidus ne soient plus autocorrélés à l'aide de la fonction ci-dessous.

```{r logwater adf test : cf. TD5}
adfTest_valid <- function(series,kmax,type){ 
# tests ADF jusqu’à des résidus non autocorrélés
k <- 0
noautocorr <- 0
while (noautocorr==0){
cat(paste0("ADF with ",k, " lags: residuals OK? "))
adf <- adfTest(series,lags=k,type=type)
pvals <- Qtests(adf@test$lm$residuals,24,fitdf=length(adf@test$lm$coefficients))[,2]
if (sum(pvals<0.05,na.rm=T) == 0) {
noautocorr <- 1; cat("OK \n")}
else cat("nope \n")
k <- k + 1
}
return(adf)
}

adf <- adfTest_valid(logwater, 24, "ct")
adf
```

On constate alors que la série en niveau n'est pas stationnaire : le test ADF ne rejette pas l'hypothèse racine unitaire (p-value > 0.05). 
On passe à la série en différence première ou intégrée d'ordre 1.

```{r dlogwater adf test}
dlogwater.source <- diff(logwater.source, 1)
dlogwater <- diff(logwater, 1)
plot(dlogwater.source, type="l", lwd=1, col="blue", main="dlogwater.source", xlab="dates", ylab="values")
summary(lm(dlogwater ~ dates[2:(length(logwater.source)-6)]))
# il n'y a pas de tendance ni de constante
adf_dlogwater <- adfTest(dlogwater, lag=0, type="nc")
Qtests(adf_dlogwater@test$lm$residuals, 24, length(adf_dlogwater@test$lm$coefficients))
```

Non seulement, la série intégrée d'ordre 1 ne présente pas de tendance et de constante significative mais en plus elle vérifie l’absence autocorrélation des résidus à l'ordre 1.

```{r dlogwater adf test}
adf <- adfTest_valid(dlogwater, 24, "nc")
adf
```

Le test ADF n'est pas rejeté avec huit retards pour la série différenciée, on retient son hypothèse alternative : la série est stationnaire. On détermine désormais pmax et qmax pour évaluer le modèle ARIMA(p,1,q) adéquat.

### Q3 - Représentation de la série avant et après transformation

``` {r plot water and dlogwater}
plot(cbind(water.source,dlogwater.source))
```

## Partie 2 : Modèles ARMA

### Q4 - Choix du modèle ARMA

On détermine d'abord pmax et qmax.

``` {r acf and pacf dlogwater}
acf(dlogwater); pacf(dlogwater)
```

qwater = 8
pwater = 9

On ignore les autocorrélations (partielles) pour les retards supérieurs à 20 dans l'objectif d'obtenir un modèle le plus simple possible.

``` {r pmax and qmax}
pmax=9; qmax=8
```

``` {r find valid models}

# fonction de test des significativités individuelles des coefficients
signif <- function(estim){
  coef <- estim$coef
  se <- sqrt(diag(estim$var.coef))
  t <- coef/se
  pval <- (1-pnorm(abs(t)))*2
  return(rbind(coef,se,pval))
}

# fonction pour estimer un arima et en vérifier l’ajustement et la validité
modelchoice <- function(p, q, data=dlogwater, k=24){
  estim <- try(arima(data, c(p,0,q),optim.control=list(maxit=20000)))
  if (class(estim)=="try-error") return(c("p"=p,"q"=q,"arsignif"=NA,"masignif"=NA,"resnocorr"=NA, "ok"=NA))
  arsignif <- if (p==0) NA else signif(estim)[3,p]<=0.05
  masignif <- if (q==0) NA else signif(estim)[3,p+q]<=0.05
  resnocorr <- sum(Qtests(estim$residuals,24,length(estim$coef)-1)[,2]<=0.05,na.rm=T)==0
  checks <- c(arsignif,masignif,resnocorr)
  ok <- as.numeric(sum(checks,na.rm=T)==(3-sum(is.na(checks))))
  return(c("p"=p,"q"=q,"arsignif"=arsignif,"masignif"=masignif,"resnocorr"=resnocorr,"ok"=ok))
}


# fonction pour estimer et vérifier tous les arma(p,q) (p<=pmax ; q<=qmax)
armamodelchoice <- function(pmax,qmax){
  pqs <- expand.grid(0:pmax,0:qmax) ; t(apply(matrix(1:dim(pqs)[1]),1,function(row) {
    p <- pqs[row,1]; q <- pqs[row,2]
    cat(paste0("Computing ARMA(",p,",",q,") \n"))
    modelchoice(p,q)
  }))}

armamodels <- armamodelchoice(pmax,qmax)

# on ne retient que les modèles ajustés et valides
selec <- armamodels[armamodels[, "ok"]==1&!is.na(armamodels[, "ok"]),]
selec
```

``` {r AIC and BIC}

# on crée la liste des arma(p,q) possibles
pqs <- apply(selec,1,function(row)
list("p"=as.numeric(row[1]), "q"=as.numeric(row[2])))
names(pqs) <- paste0("arma(", selec[,1], ",", selec[,2],")")

# on crée la liste des modèles arma(p,q)
models <- lapply(pqs, function(pq) arima(dlogwater, c(pq[["p"]], 0, pq[["q"]])))

# on calcule les AIC et BIC des modèles possibles
vapply(models, FUN.VALUE=numeric(2), function(m) c("AIC"=AIC(m),"BIC"=BIC(m)))
```

On compare les prédictions des modèles arma(1,1) (minimise l'AIC) et arma(0,1) (minimise le BIC) avec le RMSE.

``` {r select model with best rmse}

ar9ma7 <- arima(logwater, c(9,1,7), optim.control=list(maxit=20000))
ar7 <- arima(logwater, c(7,1,0), optim.control=list(maxit=20000))

# on crée des séries pour les prédictions des deux modèles à tester
models <- c("ar9ma7","ar7")
preds <- zoo(matrix(NA, ncol=2, nrow=6), order.by=tail(index(logwater.source), 6))
colnames(preds) <- models
logwaterp <- preds

# on remplit la série avec les prédictions des modèles
for (model in models){
pred <- zoo(predict(get(model), 6)$pred, order.by=tail(index(logwater.source), 6))
logwaterp[, model] <- pred
}

# on affiche les valeurs observées et les prédictions
obs <- tail(logwater.source, 6)
cbind(obs, logwaterp)

# on calcule les rmse
apply(logwaterp, 2, function(x) sqrt(sum((x-obs)^2)/6)/sd(logwater.source))
```

### Q5 - Expression du modèle ARIMA retenu.

``` {r selected model}
selected_model <- ar9ma7
selected_model
```

On vérifie la significativité du coefficient.

``` {r check selected model}
arima917 <- arima(logwater, c(9,1,7), include.mean=F)
tarima917 <- arima917$coef / sqrt(diag(arima917$var.coef))
pvarima917 <- (1-pnorm(abs(tarima917)))*2
pvarima917["ma7"]
pvarima917["ar9"]
Qtests(arima917$residuals, 24, fitdf=1)
```

## Partie III : Prévisions

### Q6 - Région de confiance pour les valeurs futures t+1 et t+2

### Q7 - Hypothèses pour la région de confiance

### Q8 - Représentation graphique de la région de confiance

``` {r forecasts}
forecasts <- forecast(ar1ma1, h=4, level=0.95)
plot(forecasts, , xlim=c(2018,2022), shadecols = "grey", fcol="blue", main = "intervalle de confiance (alpha = 0.95) pour T+1 et T+2")
```

``` {r forecasts}
prev <- forecast(arima011, h=6, level=0.95)
plot(prev, xlim=c(2017,2022))
lines(dlogwater.source[1:386], xlim=c(2017,2022))
```

### Q9 - Question ouverte


